{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b17d440",
   "metadata": {},
   "source": [
    "# Predicting Loan Payback - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87521540",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d88d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import joblib\n",
    "\n",
    "# Sklearn Imports \n",
    "from sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report,roc_auc_score\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, power_transform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb128a",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae74ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "path= r\"C:\\Users\\OLASQUARE\\Downloads\\Compressed\\playground-series-s5e11_2.zip\"\n",
    "with ZipFile(path, 'r') as zip_ref:\n",
    "    # loading the train data\n",
    "    with zip_ref.open(\"train.csv\") as train:\n",
    "        train_raw= pd.read_csv(train)\n",
    "\n",
    "    # loading the test data\n",
    "    with zip_ref.open(\"test.csv\") as test:\n",
    "        test_raw= pd.read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9ce64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   making the copy of the data\n",
    "train_data=train_raw.copy()\n",
    "test_data=test_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b221e",
   "metadata": {},
   "source": [
    "## Preprocesssing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7d5a4",
   "metadata": {},
   "source": [
    "From the EDA, there are right skewed and left skew observed.. Power transformation is recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d847a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let split the data to X and y\n",
    "X= train_data.drop(columns=['loan_paid_back','id'])\n",
    "y= train_data['loan_paid_back']\n",
    "\n",
    "# let remove the id column from test data\n",
    "test_data= test_data.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55d877eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let power transform the numerical columns to reduce skewness\n",
    "def handle_skewness(df, columns:dict):\n",
    "    df=df.copy()\n",
    "    df[columns]= power_transform(df[columns], method='yeo-johnson')\n",
    "    return df\n",
    "\n",
    "# let treat the skewness of numerical columns in X\n",
    "num_col= X.select_dtypes(include='number').columns\n",
    "X= handle_skewness(X, num_col)\n",
    "\n",
    "# let treat the skewness of numerical columns in test data too\n",
    "num_col_test= test_data.select_dtypes(include='number').columns\n",
    "test_data= handle_skewness(test_data, num_col_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ae365",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "876dfb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'marital_status', 'education_level', 'employment_status',\n",
       "       'loan_purpose', 'grade_subgrade'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col= train_data.select_dtypes(include='object').columns\n",
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4b59a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding categorical columns\n",
    "def one_hot_encoder(df, columns:dict):\n",
    "    df=df.copy()\n",
    "    df= pd.get_dummies(df, columns=columns, drop_first=True)\n",
    "    return df\n",
    "\n",
    "# based on EDA column to be encoded [gender,marital_status,employment_status]\n",
    "cat=['gender','marital_status','employment_status','loan_purpose']\n",
    "X= one_hot_encoder(X, cat)\n",
    "\n",
    "# let one hot encode the categorical columns in test data too\n",
    "test_data= one_hot_encoder(test_data, cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df9d4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual encoding target column [education_level,grade_subgrade]\n",
    "def manual_encoder(df, column:str, mapping:dict):\n",
    "    df=df.copy()\n",
    "    df[column]= df[column].map(mapping)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49555044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding education_level\n",
    "edu_order = {\n",
    "    \"High School\": 1,\n",
    "    \"Bachelor's\": 2,\n",
    "    \"Master's\": 3,\n",
    "    \"PhD\": 4,\n",
    "    \"Other\": 0\n",
    "}\n",
    "X= manual_encoder(X, 'education_level', edu_order)\n",
    "\n",
    "# let manual encode education_level in test data too\n",
    "test_data= manual_encoder(test_data, 'education_level', edu_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0cde7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecoding grade_subgrade\n",
    "grade_order = [\n",
    "    'A1','A2','A3','A4','A5',\n",
    "    'B1','B2','B3','B4','B5',\n",
    "    'C1','C2','C3','C4','C5',\n",
    "    'D1','D2','D3','D4','D5',\n",
    "    'E1','E2','E3','E4','E5',\n",
    "    'F1','F2','F3','F4','F5']\n",
    "\n",
    "grade_mapping= {grade: idx+1 for idx, grade in enumerate(grade_order)}\n",
    "X= manual_encoder(X, 'grade_subgrade', grade_mapping)\n",
    "\n",
    "# let manual encode grade_subgrade in test data too\n",
    "test_data= manual_encoder(test_data, 'grade_subgrade', grade_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ada96",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e95aa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let scale the numerical columns using minmax scaler \n",
    "scaler= MinMaxScaler()\n",
    "X[num_col]= scaler.fit_transform(X[num_col])\n",
    "\n",
    "# let scale the numerical columns in test data too\n",
    "test_data[num_col]= scaler.transform(test_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78eccec",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ea8ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features before checking multicollinearity:\n",
      "['debt_to_income_ratio', 'credit_score', 'interest_rate', 'grade_subgrade', 'employment_status_Student', 'employment_status_Unemployed']\n",
      "......................\n",
      "\n",
      "Checking multicollinearity among selected features...\n",
      "Multicollinearity detected among selected features.\n",
      "Final selected features after removing multicollinearity:\n",
      "['debt_to_income_ratio', 'credit_score', 'interest_rate', 'employment_status_Student', 'employment_status_Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# let select features based on correlation\n",
    "corr= X.corrwith(y).abs()\n",
    "corr_selected= corr[corr>0.1].index.tolist()\n",
    "\n",
    "# showing the correlation heatmap\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.heatmap(X[corr_selected].corr(), annot=True, cmap='coolwarm')\n",
    "# plt.title(\"Correlation Heatmap of Selected Features\")\n",
    "# plt.show()\n",
    "\n",
    "# checking multicoli\n",
    "# selected feature correlate with each other\n",
    "print(f\"selected features before checking multicollinearity:\\n{corr_selected}\")\n",
    "print(\"......................\\n\")\n",
    "print(\"Checking multicollinearity among selected features...\")\n",
    "if np.any(X[corr_selected].corr().abs() > 0.8):\n",
    "    print(\"Multicollinearity detected among selected features.\")\n",
    "    # dropping one of the correlated features\n",
    "    upper_tri= X[corr_selected].corr().abs().where(np.triu(np.ones(X[corr_selected].corr().shape), k=1).astype(bool))\n",
    "    to_drop= [column for column in upper_tri.columns if any(upper_tri[column]>0.8)]\n",
    "    selected_features= [feature for feature in corr_selected if feature not in to_drop]\n",
    "    print(f\"Final selected features after removing multicollinearity:\\n{selected_features}\")\n",
    "else:\n",
    "    print(\"No multicollinearity detected among selected features.\")\n",
    "    selected_features= corr_selected\n",
    "    print(f\"Final selected features:\\n{selected_features}\")\n",
    "\n",
    "\n",
    "# let save our resources for future use\n",
    "# joblib.dump(scaler, 'minmax_scaler.pkl')\n",
    "# joblib.dump(selected_features, 'selected_features.pkl')\n",
    "# joblib.dump(grade_mapping, 'grade_mapping.pkl')\n",
    "# joblib.dump(edu_order, 'edu_order.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6815b4",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7013c47",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ecfc83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let split our data\n",
    "X_final= X[selected_features]\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be0bf7",
   "metadata": {},
   "source": [
    "### Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94c6e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize models\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=234),\n",
    "#     \"Random Forest\": RandomForestClassifier(random_state=234),\n",
    "#     # \"Decision Tree\": DecisionTreeClassifier(random_state=234),\n",
    "#     # \"KNN\": KNeighborsClassifier()\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f64e7",
   "metadata": {},
   "source": [
    "### Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42715c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let train those models\n",
    "\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     model.fit(X_train, y_train)\n",
    "#     # save the model for future use\n",
    "#     joblib.dump(model, f'{name}.pkl')\n",
    "\n",
    "#     # model.dump(f\"{name}.pkl\")\n",
    "#     # y_val_pred= model.predict(X_val)\n",
    "\n",
    "#     # roc_auc= roc_auc_score(y_val, model.predict_proba(X_val)[:,1])\n",
    "#     # print(f\"{name} Validation ROC-AUC Score: {roc_auc:.4f}\")\n",
    "#     # print(classification_report(y_val, y_val_pred))\n",
    "#     # print(\"==\"*50)\n",
    "#     # print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efb257",
   "metadata": {},
   "source": [
    "### Tunning the best trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e43ac",
   "metadata": {},
   "source": [
    "Since Logistic Regression and Random Forest are very close. So, I will tune Random Forest for stability and interpretability of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "125f5dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 12}\n",
      "Best CV AUC: 0.9137\n"
     ]
    }
   ],
   "source": [
    "# let tune the best trained Model using Random search\n",
    "rf = RandomForestClassifier(random_state=234, n_jobs=-1)\n",
    "\n",
    "# parameter random for tuning\n",
    "# new changes\n",
    "param_dist = {\n",
    "    'n_estimators': [100,200,400],\n",
    "    'max_depth': [8, 12, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# new Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=8,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    random_state=234,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# Fit on your training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best CV AUC: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# save the best model\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f93cec",
   "metadata": {},
   "source": [
    "### Evaluating the best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f85a98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let do cross validation for the best estimator\n",
    "# cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "# print(f\"Cross-validation scores for Random Forest: {cv_scores}\")\n",
    "# print(f\"Mean Cross-validation score: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1c9b8",
   "metadata": {},
   "source": [
    "### Predict with Best tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc30bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  loan_paid_back\n",
      "0  593994        0.918884\n",
      "1  593995        0.982649\n",
      "2  593996        0.475085\n",
      "3  593997        0.942308\n",
      "4  593998        0.951999\n",
      "5  593999        0.973974\n",
      "6  594000        0.983233\n",
      "7  594001        0.969953\n",
      "8  594002        0.933707\n",
      "9  594003        0.008908\n"
     ]
    }
   ],
   "source": [
    "# Predict with Best tuned model\n",
    "test_df=test_data[selected_features] # selecting only the features used in training\n",
    "\n",
    "# predicting with the best model\n",
    "y_test_pred= best_model.predict_proba(test_df)[:, 1]\n",
    "\n",
    "# preparing submission file\n",
    "submission= pd.DataFrame({\n",
    "    'id': test_raw['id'],\n",
    "    'loan_paid_back': y_test_pred\n",
    "})\n",
    "print(submission.head(10))\n",
    "submission.to_csv('loan_payback_predictions3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
